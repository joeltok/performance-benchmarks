{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2dd2a4-d557-407d-8060-70a2c48b0356",
   "metadata": {},
   "source": [
    "# Python DataFrames vs Native Data Structures - Memory Consumption\n",
    "\n",
    "In this experiment, we want to evaluate the memory footprint in Python 3 of data stored in various tabular formats. In particular, we want to compare DataFrames, List of Dictionaries and Dictionaries of Lists.\n",
    "\n",
    "These 3 are different ways to store table-like data, which basically can be seen as data represented by rows and columns. In this examination, we will ignore any questions regarding efficient read/write or lookups. We are purely concerned with one question today: which approach will be more memory intensive? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7548e6d-6473-4039-bfbe-2d906f3a6cd5",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "\n",
    "We generate a nonsense (but very large) simple test dataset for this experiment, using a list of some popular dog breeds. Note that this list is definitely not biased at all, and all of them are definitely dogs.\n",
    "\n",
    "To ensure that the test is general enough for most use cases, we ensure that this dataset has at least three primitive data types: str, int and float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6574c1d-608a-456b-896d-35ef2bd8ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "breeds = [\n",
    "    'German Shepherd',\n",
    "    'Golden Retriever',\n",
    "    'Siberian Husky',\n",
    "    'Japanese Spitz',\n",
    "    'Sleeping Whippets',\n",
    "    'Samoyed',\n",
    "    'Hippogriff',\n",
    "    'Standard Poodle',\n",
    "    'Wallaby',\n",
    "    'Dalmatian',\n",
    "    'Dachshund',\n",
    "    'Flying Fox',\n",
    "    'Mandrake',\n",
    "]\n",
    "\n",
    "list_of_dictionaries = [ {\n",
    "    'breed': random.choice(breeds),\n",
    "    'count': random.randrange(0,200), \n",
    "    'barks': random.uniform(50,70),\n",
    "} for _ in range(1000000) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e9cb7-af51-40c6-a3b7-5ad0a027a4f4",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "Measuring the memory of DataFrames is relatively simple, and can be done with a simple built-in function: `DataFrame.memory_usage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06b00782-a910-41da-b7d9-bed08918d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84,613,093 B\n",
      "84.61 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list_of_dictionaries)\n",
    "mem = df.memory_usage(index=True, deep=True)\n",
    "total_size = sum(mem)\n",
    "\n",
    "print(f'{total_size:,} B')\n",
    "print(f'{total_size/1000000:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90a40a-c77c-4b5d-b0e0-a239b89cbcb9",
   "metadata": {},
   "source": [
    "## List of Dictionaries\n",
    "\n",
    "Measuring lists of dictionaries is slightly more involved. This is because the memory footprint of lists and dictionaries do not include the memory taken up by the objects in them. As such, one has to iterate each object within the list, and each key-value pair within each dictionary to get the proper values. \n",
    "\n",
    "We will use the function `sys.getsizeof` to assist us in getting the memory footprint of individual object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79836fc7-6ab5-42a3-ab42-569fb1ac657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515,041,877 B\n",
      "515.04 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "total_size = 0\n",
    "total_size += sys.getsizeof(list_of_dictionaries)\n",
    "for dictionary in list_of_dictionaries:\n",
    "    total_size += sys.getsizeof(dictionary)\n",
    "    for key, value in zip(dictionary.keys(), dictionary.values()):\n",
    "            total_size += sys.getsizeof(key)\n",
    "            total_size += sys.getsizeof(value)\n",
    "\n",
    "print(f'{total_size:,} B')\n",
    "print(f'{total_size/1000000:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b639e2",
   "metadata": {},
   "source": [
    "## Dictionary of Lists\n",
    "\n",
    "An alternative way of representing tabular data in json format is the dictionary of lists. It consists of a dictionary, where each key represents a column, and points to an array. Each array's index in such a case corresponds to a row index. \n",
    "\n",
    "An e.g. of such a data structure is as follows:\n",
    "```json\n",
    "{\n",
    "    'breed': [ ... ],\n",
    "    'count': [ ... ],\n",
    "    'barks': [ ... ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1690a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136,593,711 B\n",
      "136.59 MB\n"
     ]
    }
   ],
   "source": [
    "dictionary_of_lists = df.to_dict(orient='list')\n",
    "total_size = 0\n",
    "total_size += sys.getsizeof(dictionary_of_lists)\n",
    "for key in dictionary_of_lists.keys():\n",
    "    total_size += sys.getsizeof(key)\n",
    "    total_size += sys.getsizeof(dictionary_of_lists[key])\n",
    "    for item in dictionary_of_lists[key]:\n",
    "        total_size += sys.getsizeof(item)\n",
    "\n",
    "print(f'{total_size:,} B')\n",
    "print(f'{total_size/1000000:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2255965-c48b-4894-b6e8-0e6f641460e4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that the Pandas DataFrames, despite their added complexity, have a significantly smaller footprint than lists of dictionaries (~6 times smaller), and even a dictionary of lists (~2 times smaller). We can conclude hence that the use of DataFrames can be a useful non-trivial optimisation in certain use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da8660-2049-4c37-99d0-a8b8dd803cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
